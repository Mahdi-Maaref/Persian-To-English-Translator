
<div align="center">
 
# ğŸŒ Persian-To-English-Translator
![Banner](banner.png)
### Lightweight, Fast, and Accurate Neural Machine Translation Models

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Model-orange)](https://huggingface.co/)
[![Unsloth](https://img.shields.io/badge/âš¡-Unsloth-purple)](https://github.com/unslothai/unsloth)
[![Models: Qwen3](https://img.shields.io/badge/Base%20Models-Qwen3-blue)](https://huggingface.co/Qwen)
[![Framework: Unsloth](https://img.shields.io/badge/Framework-Unsloth-green)](https://github.com/unslothai/unsloth)
[![Dataset: 300k](https://img.shields.io/badge/Dataset-300k%20Pairs-red)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

**Two fine-tuned Persian to English translation models: one ultra-lightweight for speed, one larger for maximum accuracy â€” both optimized for efficiency.**

[Models](#-available-models) â€¢
[Comparison](#-model-comparison--benchmarks) â€¢
[Features](#-features) â€¢
[Installation](#-installation) â€¢
[Quick Start](#-quick-start) â€¢
[Training](#-training-details) â€¢
[Dataset](#-dataset) â€¢
[License](#-license)

## ğŸ“– Project Overview

**Persian-To-English-Translator** offers two specialized machine translation models designed to bridge the gap between Persian (Farsi) and English. We provide options for different use cases:

| ğŸª¶ **Lite Model** | ğŸ¯ **Pro Model** |
|:---:|:---:|
| Qwen3-0.6B | Qwen3-4B |
| Ultra-fast, minimal resources | Higher accuracy, still lightweight |
| Perfect for edge devices | Perfect for quality-focused apps |

### ğŸš€ Key Goals

- **Flexibility:** Choose between speed-optimized or accuracy-optimized models
- **Low-Resource:** Both models run efficiently on consumer-grade hardware
- **High Speed:** Fast inference suitable for real-time applications
- **Accuracy:** Fine-tuned on high-quality, cleaned datasets to handle Persian-English nuances

</div>

---

## ğŸ”¥ Available Models

### Model Specifications

| Feature | ğŸª¶ **Lite (0.6B)** | ğŸ¯ **Pro (4B)** |
|---------|:---:|:---:|
| **Base Model** | Qwen3-0.6B | Qwen3-4B |
| **Total Parameters** | 616M | ~4B |
| **Trainable Params** | 20.2M (3.28%) | TBD |
| **Model Size (FP16)** | ~1.2GB | ~8GB |
| **GGUF Q4_K_M Size** | ~400MB | ~2.5GB |
| **Inference Speed** | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡ |
| **Translation Quality** | â­â­â­â­ | â­â­â­â­â­ |
| **RAM Required** | ~2GB | ~6GB |
| **GPU Required** | Optional | Recommended |
| **Best For** | Mobile, Edge, Real-time | Desktop, Server, Quality |

### ğŸª¶ Lite Model (0.6B) â€” Speed Champion
```
Perfect for: Mobile apps, IoT devices, real-time translation, 
             low-power devices, batch processing at scale
```

### ğŸ¯ Pro Model (4B) â€” Accuracy Champion
```
Perfect for: Professional translation, content creation, 
             complex sentences, idiomatic expressions, nuanced text
```

> ğŸ’¡ **Note:** Even our "Pro" 4B model is remarkably lightweight compared to industry giants like GPT-4 (1.7T params) or LLaMA-70B. It's like comparing a feather to an elephant!

---

## ğŸ“Š Model Comparison & Benchmarks

### ğŸ† Performance Comparison: Fine-tuned vs Base Models vs Google Translate

#### Quantitative Metrics

| Model | BLEU â†‘ | chrF â†‘ | COMET â†‘ | Tokens/sec (GPU) â†‘ | Tokens/sec (CPU) â†‘ |
|:------|:------:|:------:|:-------:|:------------------:|:------------------:|
| **ğŸª¶ Lite (Fine-tuned)** | TBD | TBD | TBD | TBD | TBD |
| Qwen3-0.6B (Base) | TBD | TBD | TBD | TBD | TBD |
| **ğŸ¯ Pro (Fine-tuned)** | TBD | TBD | TBD | TBD | TBD |
| Qwen3-4B (Base) | TBD | TBD | TBD | TBD | TBD |
| Google Translate | TBD | TBD | TBD | N/A | N/A |

> ğŸš§ **Note:** Comprehensive benchmarks are in progress. Results will be updated soon.

#### Improvement Over Base Models

| Comparison | BLEU Improvement | chrF Improvement | Notes |
|:-----------|:----------------:|:----------------:|:------|
| ğŸª¶ Lite vs Qwen3-0.6B Base | +TBD% | +TBD% | Specialized for FAâ†’EN |
| ğŸ¯ Pro vs Qwen3-4B Base | +TBD% | +TBD% | Higher baseline, more gains |
| ğŸª¶ Lite vs Google Translate | TBD | TBD | Offline capable! |
| ğŸ¯ Pro vs Google Translate | TBD | TBD | Privacy-preserving! |

---

### ğŸ“ Qualitative Comparison: Translation Examples

#### Example 1: Simple Sentence
| Source (Persian) | Ø³Ù„Ø§Ù…ØŒ Ø­Ø§Ù„ Ø´Ù…Ø§ Ú†Ø·ÙˆØ± Ø§Ø³ØªØŸ |
|:-----------------|:------------------------|
| **ğŸª¶ Lite (Fine-tuned)** | Hello, how are you? |
| Qwen3-0.6B (Base) | TBD |
| **ğŸ¯ Pro (Fine-tuned)** | Hello, how are you doing? |
| Qwen3-4B (Base) | TBD |
| Google Translate | Hello, how are you? |
| **Reference** | Hello, how are you? |

#### Example 2: Complex/Idiomatic Sentence
| Source (Persian) | Ø§Ø² Ú©ÙˆØ²Ù‡ Ù‡Ù…Ø§Ù† Ø¨Ø±ÙˆÙ† ØªØ±Ø§ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ø§ÙˆØ³Øª |
|:-----------------|:-----------------------------------|
| **ğŸª¶ Lite (Fine-tuned)** | TBD |
| Qwen3-0.6B (Base) | TBD |
| **ğŸ¯ Pro (Fine-tuned)** | TBD |
| Qwen3-4B (Base) | TBD |
| Google Translate | TBD |
| **Reference** | What's bred in the bone comes out in the flesh |

#### Example 3: Formal/Literary Text
| Source (Persian) | Ø¯Ø± Ø§ÛŒÙ† Ø±Ø§Ø³ØªØ§ØŒ ØªÙˆØ³Ø¹Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù‡Ø¯Ø§Ù Ø§ØµÙ„ÛŒ Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª |
|:-----------------|:-----------------------------------------------------------------------------|
| **ğŸª¶ Lite (Fine-tuned)** | TBD |
| Qwen3-0.6B (Base) | TBD |
| **ğŸ¯ Pro (Fine-tuned)** | TBD |
| Qwen3-4B (Base) | TBD |
| Google Translate | TBD |
| **Reference** | In this regard, sustainable development has been considered as one of the main objectives |

#### Example 4: Colloquial/Informal Text
| Source (Persian) | Ø¯Ø§Ø¯Ø§Ø´ Ø§ÛŒÙ† Ú©Ø§Ø± Ø®ÛŒÙ„ÛŒ Ø³Ø®ØªÙ‡ØŒ ÙˆÙ„Ø´ Ú©Ù† Ø¨ÛŒØ®ÛŒØ§Ù„ |
|:-----------------|:--------------------------------------|
| **ğŸª¶ Lite (Fine-tuned)** | TBD |
| Qwen3-0.6B (Base) | TBD |
| **ğŸ¯ Pro (Fine-tuned)** | TBD |
| Qwen3-4B (Base) | TBD |
| Google Translate | TBD |
| **Reference** | Bro, this is too hard, just forget about it |

---

### ğŸ“ˆ Comparison Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSLATION QUALITY RADAR                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚   Category              Lite(FT)  Base-0.6B  Pro(FT)  Base-4B   Google    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚   Simple Sentences      â˜…â˜…â˜…â˜…â˜…     â˜…â˜…â˜…â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜…â˜†     â˜…â˜…â˜…â˜…â˜…     â”‚
â”‚   Complex Sentences     â˜…â˜…â˜…â˜…â˜†     â˜…â˜…â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜…â˜…â˜†     â”‚
â”‚   Idioms & Proverbs     â˜…â˜…â˜…â˜†â˜†     â˜…â˜†â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜†â˜†â˜†     â˜…â˜…â˜…â˜†â˜†     â”‚
â”‚   Formal Text           â˜…â˜…â˜…â˜…â˜†     â˜…â˜…â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜…â˜…â˜†     â”‚
â”‚   Colloquial Text       â˜…â˜…â˜…â˜…â˜†     â˜…â˜†â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜†â˜†â˜†     â˜…â˜…â˜…â˜†â˜†     â”‚
â”‚   Technical Terms       â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜†    â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜…â˜…â˜…     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚   Speed                 â˜…â˜…â˜…â˜…â˜…     â˜…â˜…â˜…â˜…â˜…      â˜…â˜…â˜…â˜†â˜†    â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜…â˜…â˜†     â”‚
â”‚   Offline Capability    â˜…â˜…â˜…â˜…â˜…     â˜…â˜…â˜…â˜…â˜…      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜…â˜…     â˜†â˜†â˜†â˜†â˜†     â”‚
â”‚   Privacy               â˜…â˜…â˜…â˜…â˜…     â˜…â˜…â˜…â˜…â˜…      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜…â˜…     â˜…â˜†â˜†â˜†â˜†     â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FT = Fine-Tuned
```

---

### ğŸ†š Why Choose Our Models Over Alternatives?

| Feature | ğŸª¶ Lite | ğŸ¯ Pro | Base Models | Google Translate |
|:--------|:------:|:------:|:-----------:|:----------------:|
| **Persian-Optimized** | âœ… | âœ… | âŒ | âœ… |
| **Offline Usage** | âœ… | âœ… | âœ… | âŒ |
| **Privacy-Preserving** | âœ… | âœ… | âœ… | âŒ |
| **Low Latency** | âœ… | âœ… | âœ… | âŒ |
| **No API Costs** | âœ… | âœ… | âœ… | âŒ |
| **Customizable** | âœ… | âœ… | âœ… | âŒ |
| **Idiomatic Understanding** | âœ… | âœ… | âŒ | Partial |
| **Edge Deployment** | âœ… | âš ï¸ | âœ… | âŒ |
| **Open Source** | âœ… | âœ… | âœ… | âŒ |

---

## âœ¨ Features

- ğŸª¶ **Dual Options** â€” Choose between ultra-lite (0.6B) or balanced (4B) models
- âš¡ **Fast Inference** â€” Optimized with Unsloth for 2x faster performance
- ğŸ¯ **High Accuracy** â€” Fine-tuned on 300K high-quality sentence pairs
- ğŸ’¾ **Low Resource** â€” Both models run on consumer hardware
- ğŸ“¦ **Multiple Formats** â€” Available in GGUF format for local deployment
- ğŸ”“ **Open Source** â€” MIT licensed for maximum flexibility
- ğŸ”„ **Scalable** â€” Pick the right model for your resource constraints
- ğŸ”’ **Privacy** â€” All processing happens locally, no data sent to cloud
- ğŸŒ **Offline** â€” Works without internet connection

---

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/Mahdi-Maaref/Persian-To-English-Translator.git
cd Persian-To-English-Translator

# Install dependencies
pip install torch transformers accelerate
pip install unsloth peft
```

---

## ğŸš€ Quick Start

### Using Transformers

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Choose your model:
# Lite (0.6B) - Fast & Lightweight
model_name = "Mahdi-Maaref/Persian-To-English-Translator-Lite"

# Pro (4B) - Higher Accuracy
# model_name = "Mahdi-Maaref/Persian-To-English-Translator-Pro"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Translate
persian_text = "Ø³Ù„Ø§Ù…ØŒ Ø­Ø§Ù„ Ø´Ù…Ø§ Ú†Ø·ÙˆØ± Ø§Ø³ØªØŸ"
prompt = f"Translate the following Persian text to English:\n{persian_text}\nEnglish:"

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=128)
translation = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(translation)
```

### Using GGUF (llama.cpp)

```bash
# Download GGUF model (Lite version)
wget https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Lite-GGUF/resolve/main/model-q4_k_m.gguf

# Or Pro version
wget https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Pro-GGUF/resolve/main/model-q4_k_m.gguf

# Run with llama.cpp
./main -m model-q4_k_m.gguf -p "Translate Persian to English: Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§"
```

### Model Selection Guide

```python
# Use this helper to choose the right model
def select_model(priority="balanced"):
    """
    priority options:
    - "speed": Use Lite model (0.6B)
    - "quality": Use Pro model (4B)  
    - "balanced": Use Lite for simple, Pro for complex text
    """
    if priority == "speed":
        return "Mahdi-Maaref/Persian-To-English-Translator-Lite"
    elif priority == "quality":
        return "Mahdi-Maaref/Persian-To-English-Translator-Pro"
    else:
        # Implement your logic here
        pass
```

---

## ğŸ“Š Training Details

### ğŸª¶ Lite Model (Qwen3-0.6B) Training

```
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 290,376 | Num Epochs = 2 | Total steps = 72,594
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,185,088 of 616,235,008 (3.28% trained)
```

### ğŸ¯ Pro Model (Qwen3-4B) Training

```
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 290,376 | Num Epochs = 2 | Total steps = TBD
O^O/ \_/ \    Batch size per device = TBD | Gradient accumulation steps = TBD
\        /    Data Parallel GPUs = 1 | Total batch size = TBD
 "-____-"     Trainable parameters = TBD of ~4,000,000,000 (TBD% trained)
```

### Hyperparameters Comparison

| Parameter | ğŸª¶ Lite (0.6B) | ğŸ¯ Pro (4B) |
|-----------|:---:|:---:|
| **Base Model** | Qwen3-0.6B | Qwen3-4B |
| **Fine-tuning Method** | PEFT LoRA | PEFT LoRA |
| **Optimizer** | Unsloth | Unsloth |
| **Number of Epochs** | 2 | 2 |
| **Total Training Steps** | 72,594 | TBD |
| **Batch Size per Device** | 2 | TBD |
| **Gradient Accumulation Steps** | 4 | TBD |
| **Effective Batch Size** | 8 | TBD |
| **Trainable Parameters** | 20.2M (3.28%) | TBD |
| **Total Parameters** | 616M | ~4B |
| **Training Examples** | 290,376 | 290,376 |

### Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Persian-To-English-Translator               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ğŸª¶ LITE MODEL          â”‚        ğŸ¯ PRO MODEL                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Qwen3-0.6B (Base)       â”‚       Qwen3-4B (Base)              â”‚
â”‚  + LoRA Adapters           â”‚     + LoRA Adapters                â”‚
â”‚  + Unsloth Optimizations   â”‚     + Unsloth Optimizations        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Trainable: 20.2M (3.28%)  â”‚     Trainable: TBD                 â”‚
â”‚  Total: 616M params        â”‚     Total: ~4B params              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ’¨ Speed: â˜…â˜…â˜…â˜…â˜…           â”‚     ğŸ’¨ Speed: â˜…â˜…â˜…â˜†â˜†                â”‚
â”‚  ğŸ¯ Quality: â˜…â˜…â˜…â˜…â˜†         â”‚     ğŸ¯ Quality: â˜…â˜…â˜…â˜…â˜…              â”‚
â”‚  ğŸ’¾ Size: â˜…â˜…â˜…â˜…â˜…            â”‚     ğŸ’¾ Size: â˜…â˜…â˜…â˜…â˜†                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Dataset

### Overview

Both models were trained on **300,000 high-quality Persian-English sentence pairs**, carefully curated and cleaned to address specific challenges in Persian-to-English translation.

### Dataset Quality Comparison

| Dataset | Size | Quality | Persian-Specific | Cleaned | Notes |
|---------|------|---------|------------------|---------|-------|
| **Ours (Used)** | 300K | â­â­â­â­â­ | âœ… Yes | âœ… Yes | Curated for FAâ†’EN challenges |
| OPUS-100 | 1M+ | â­â­â­ | âŒ No | âŒ No | Generic multilingual |
| CCAligned | 500K+ | â­â­ | âŒ No | âŒ No | Noisy web crawl |
| WikiMatrix | 200K | â­â­â­ | âŒ No | Partial | Wikipedia only |
| TED2020 | 50K | â­â­â­â­ | âŒ No | âœ… Yes | Limited domain |

### Data Cleaning Pipeline

```
Raw Data â†’ Deduplication â†’ Length Filter â†’ Quality Filter â†’ Final Dataset
   â”‚              â”‚              â”‚              â”‚              â”‚
  500K+        450K           380K           320K           300K
```

### Persian-Specific Challenges Addressed

- âœ… Right-to-Left (RTL) text handling
- âœ… Persian-specific characters and diacritics
- âœ… Informal/colloquial expressions
- âœ… Persian idioms and proverbs
- âœ… Mixed Persian-Arabic script
- âœ… Transliteration of names and places

---

## ğŸ“¦ Model Checkpoints

### ğŸª¶ Lite Model (0.6B)

| Format | Size | Use Case | Download |
|--------|------|----------|----------|
| Full Model | ~1.2GB | Training/Fine-tuning | [ğŸ¤— Hub](https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Lite) |
| GGUF Q4_K_M | ~400MB | Fast CPU Inference | [ğŸ¤— Hub](https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Lite-GGUF) |
| GGUF Q8_0 | ~650MB | Balanced Quality/Speed | [ğŸ¤— Hub](https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Lite-GGUF) |
| GGUF F16 | ~1.2GB | Maximum Quality | [ğŸ¤— Hub](https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Lite-GGUF) |

### ğŸ¯ Pro Model (4B)

| Format | Size | Use Case | Download |
|--------|------|----------|----------|
| Full Model | ~8GB | Training/Fine-tuning | [ğŸ¤— Hub](https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Pro) |
| GGUF Q4_K_M | ~2.5GB | Fast CPU Inference | [ğŸ¤— Hub](https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Pro-GGUF) |
| GGUF Q8_0 | ~4.5GB | Balanced Quality/Speed | [ğŸ¤— Hub](https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Pro-GGUF) |
| GGUF F16 | ~8GB | Maximum Quality | [ğŸ¤— Hub](https://huggingface.co/Mahdi-Maaref/Persian-To-English-Translator-Pro-GGUF) |

---

## ğŸ¯ Which Model Should I Use?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL SELECTION GUIDE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ğŸ“± Mobile App / Edge Device?          â†’ ğŸª¶ Lite (0.6B)         â”‚
â”‚  ğŸ–¥ï¸ Desktop / Server?                  â†’ ğŸ¯ Pro (4B)            â”‚
â”‚  âš¡ Real-time Translation?              â†’ ğŸª¶ Lite (0.6B)         â”‚
â”‚  ğŸ“ Professional Content?              â†’ ğŸ¯ Pro (4B)            â”‚
â”‚  ğŸ’° Limited GPU Memory (<4GB)?         â†’ ğŸª¶ Lite (0.6B)         â”‚
â”‚  ğŸ¨ Complex/Idiomatic Text?            â†’ ğŸ¯ Pro (4B)            â”‚
â”‚  ğŸ“Š Batch Processing at Scale?         â†’ ğŸª¶ Lite (0.6B)         â”‚
â”‚  ğŸ“– High-Quality Publication?          â†’ ğŸ¯ Pro (4B)            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 Mahdi Maaref

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...
```

---

## ğŸ™ Acknowledgments

- [Qwen Team](https://github.com/QwenLM) for the excellent base models
- [Unsloth](https://github.com/unslothai/unsloth) for training optimizations
- [Hugging Face](https://huggingface.co/) for the transformers library
- The Persian NLP community for valuable resources and feedback

---

<div align="center">

### â­ Star this repo if you find it useful!

**Made with â¤ï¸ for the Persian NLP Community**

[![GitHub stars](https://img.shields.io/github/stars/Mahdi-Maaref/Persian-To-English-Translator?style=social)](https://github.com/Mahdi-Maaref/Persian-To-English-Translator)

---

### ğŸª¶ Lite for Speed | ğŸ¯ Pro for Precision

*Both still lighter than a typical browser tab! ğŸš€*

</div>
