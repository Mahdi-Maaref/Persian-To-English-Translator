<div align="center">
 
# ğŸŒ Persian-To-English-Translator
![Banner](banner.png)
### Lightweight, Fast, and Accurate Neural Machine Translation Models

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Model-orange)](https://huggingface.co/)
[![Unsloth](https://img.shields.io/badge/âš¡-Unsloth-purple)](https://github.com/unslothai/unsloth)
[![Models: Qwen3](https://img.shields.io/badge/Base%20Models-Qwen3-blue)](https://huggingface.co/Qwen)
[![Framework: Unsloth](https://img.shields.io/badge/Framework-Unsloth-green)](https://github.com/unslothai/unsloth)
[![Dataset: 300k](https://img.shields.io/badge/Dataset-300k%20Pairs-red)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
### â­ Lite for Speed | ğŸ¯ Pro for Precision

---

**Two fine-tuned Persian to English translation models: one ultra-lightweight for speed, one larger for maximum accuracy â€” both optimized for efficiency.**

[Models](#-available-models) â€¢
[Comparison](#-model-comparison--benchmarks) â€¢
[Features](#-features) â€¢
[Installation](#-installation) â€¢
[Quick Start](#-quick-start) â€¢
[Training](#-training-details) â€¢
[Dataset](#-dataset) â€¢
[License](#-license)

</div>

---

## ğŸ“– Project Overview

<div align="center">

**Nava** (Ù†ÙˆØ§) offers two specialized machine translation models designed to bridge the gap between Persian (Farsi) and English. These models are tailored for different use cases, providing a choice between extreme speed and maximum accuracy.

| ğŸ¯ Nava Lite | ğŸ¯ Nava Pro |
|:---:|:---:|
| Based on Qwen3-0.6B | Based on Qwen3-4B |
| Ultra-fast, minimal resources | Higher accuracy, still efficient |
| Perfect for edge devices | Perfect for quality-focused apps |

</div>

### ğŸš€ Key Goals

- **Flexibility:** Choose between a speed-optimized model...
- **Low-Resource:** Both models are designed to run efficiently...
- **High Speed:** Optimized for fast inference...
- **Accuracy:** Fine-tuned on high-quality datasets...

---

## ğŸ”¥ Available Models

### Model Specifications

| Feature | ğŸ¯ **Nava Lite (0.6B)** | ğŸ¯ **Nava Pro (4B)** |
|---------|:---:|:---:|
| **Base Model** | Qwen3-0.6B | Qwen3-4B |
| **Total Parameters** | 616M | ~4B |
| **Trainable Params** | 20.2M (3.28%) | TBD |
| **Model Size (FP16)** | ~1.2GB | ~8GB |
| **GGUF Q4_K_M Size** | ~400MB | ~2.5GB |
| **Inference Speed** | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡ |
| **Translation Quality** | â­â­â­â­ | â­â­â­â­â­ |
| **RAM Required** | ~2GB | ~6GB |
| **GPU Required** | Optional | Recommended |
| **Best For** | Mobile, Edge, Real-time | Desktop, Server, Quality |

### ğŸ¯ Lite Model (0.6B) â€” Speed Champion
```
Perfect for: Mobile apps, IoT devices, real-time translation, 
             low-power devices, batch processing at scale
```

### ğŸ¯ Pro Model (4B) â€” Accuracy Champion
```
Perfect for: Professional translation, content creation, 
             complex sentences, idiomatic expressions, nuanced text
```

> ğŸ’¡ **Note:** Even our "Pro" 4B model is remarkably lightweight compared to industry giants like GPT-4 (1.7T params) or LLaMA-70B. It's like comparing a feather to an elephant!

---

## ğŸ“Š Model Comparison & Benchmarks

### ğŸ† Performance Comparison: Fine-tuned vs Base Models vs Google Translate

#### ğŸ“ˆ Quantitative Metrics

| Model | BLEU â†‘ | chrF â†‘ | COMET â†‘ | Tokens/sec (GPU) â†‘ | Tokens/sec (CPU) â†‘ |
|:------|:------:|:------:|:-------:|:------------------:|:------------------:|
| **ğŸª¶ Lite (Fine-tuned)** | TBD | TBD | TBD | TBD | TBD |
| Qwen3-0.6B (Base) | TBD | TBD | TBD | TBD | TBD |
| **ğŸ¯ Pro (Fine-tuned)** | TBD | TBD | TBD | TBD | TBD |
| Qwen3-4B (Base) | TBD | TBD | TBD | TBD | TBD |
| Google Translate | TBD | TBD | TBD | N/A | N/A |

> ğŸš§ **Note:** Comprehensive benchmarks are in progress. Results will be updated soon.

#### ğŸ“Š Improvement Over Base Models

| Comparison | BLEU Improvement | chrF Improvement | Notes |
|:-----------|:----------------:|:----------------:|:------|
| ğŸª¶ Lite vs Qwen3-0.6B Base | +TBD% | +TBD% | Specialized for FAâ†’EN |
| ğŸ¯ Pro vs Qwen3-4B Base | +TBD% | +TBD% | Higher baseline, more gains |
| ğŸª¶ Lite vs Google Translate | TBD | TBD | Offline capable! |
| ğŸ¯ Pro vs Google Translate | TBD | TBD | Privacy-preserving! |

#### ğŸ“ Qualitative Comparison

| Challenge Type | Source (Persian) | Qwen-0.6B (Base) | Google Translate | Ours (Fine-tuned) |
| :--- | :--- | :--- | :--- | :--- |
| **Simple** | Ø³Ù„Ø§Ù…ØŒ Ø­Ø§Ù„ Ø´Ù…Ø§ Ú†Ø·ÙˆØ± Ø§Ø³ØªØŸ | TBD | Hello, how are you? | Hello, how are you? |
| **Idiomatic** | Ø§Ø² Ú©ÙˆØ²Ù‡ Ù‡Ù…Ø§Ù† Ø¨Ø±ÙˆÙ† ØªØ±Ø§ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ø§ÙˆØ³Øª | TBD | TBD | TBD |
| **Formal** | Ø¯Ø± Ø§ÛŒÙ† Ø±Ø§Ø³ØªØ§ØŒ ØªÙˆØ³Ø¹Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù‡Ø¯Ø§Ù Ø§ØµÙ„ÛŒ Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª | TBD | TBD | TBD |
| **Colloquial** | Ø¯Ø§Ø¯Ø§Ø´ Ø§ÛŒÙ† Ú©Ø§Ø± Ø®ÛŒÙ„ÛŒ Ø³Ø®ØªÙ‡ØŒ ÙˆÙ„Ø´ Ú©Ù† Ø¨ÛŒØ®ÛŒØ§Ù„ | TBD | TBD | TBD |

#### ğŸ“ˆ Comparison Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSLATION QUALITY RADAR                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                        â”‚
â”‚   Category              Lite(FT)     Base-0.6B     Pro(FT)    Base-4B      Google      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚   Simple Sentences      â˜…â˜…â˜…â˜…â˜…     â˜…â˜…â˜…â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜…â˜†     â˜…â˜…â˜…â˜…â˜…     â”‚
â”‚   Complex Sentences     â˜…â˜…â˜…â˜…â˜†     â˜…â˜…â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜…â˜…â˜†     â”‚
â”‚   Idioms & Proverbs     â˜…â˜…â˜…â˜†â˜†     â˜…â˜†â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜†â˜†â˜†     â˜…â˜…â˜…â˜†â˜†     â”‚
â”‚   Formal Text           â˜…â˜…â˜…â˜…â˜†     â˜…â˜…â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜…â˜…â˜†     â”‚
â”‚   Colloquial Text       â˜…â˜…â˜…â˜…â˜†     â˜…â˜†â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜†â˜†â˜†     â˜…â˜…â˜…â˜†â˜†     â”‚
â”‚   Technical Terms       â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜†â˜†â˜†      â˜…â˜…â˜…â˜…â˜†    â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜…â˜…â˜…     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚   Speed                 â˜…â˜…â˜…â˜…â˜…     â˜…â˜…â˜…â˜…â˜…      â˜…â˜…â˜…â˜†â˜†    â˜…â˜…â˜…â˜†â˜†     â˜…â˜…â˜…â˜…â˜†     â”‚
â”‚   Offline Capability    â˜…â˜…â˜…â˜…â˜…     â˜…â˜…â˜…â˜…â˜…      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜…â˜…     â˜†â˜†â˜†â˜†â˜†     â”‚
â”‚   Privacy               â˜…â˜…â˜…â˜…â˜…     â˜…â˜…â˜…â˜…â˜…      â˜…â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜…â˜…     â˜…â˜†â˜†â˜†â˜†     â”‚
â”‚                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FT = Fine-Tuned
```



## ğŸ› ï¸ Installation

You can use the `inference.py` script from this repository on your local system, Google Colab, or cloud computing services.

### ğŸ“‹ Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (recommended but not required)

### ğŸ–¥ï¸ Local System

**Recommended:** Create a virtual environment to avoid dependency conflicts:

### â˜ï¸ Google Colab

**Note:** Recently, Google Colab has become extremely slow for Iranian users and requires VPN.


### ğŸ³ Docker (Recommended for Cloud/Production)

If you encounter software conflicts on local or cloud environments, use Docker:

```bash
https://hub.docker.com/r/unsloth/unsloth
```

---

## ğŸš€ Quick Start

```python
import re
from unsloth import FastLanguageModel

model_id = "MahdiMaaref/Persian-To-English-Translator"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_id,
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)
FastLanguageModel.for_inference(model)

# Example translation
persian_text = "Ø§Ù…Ø±ÙˆØ² Ø¨Ø¹Ø¯ Ø§Ø² Ú©Ø§Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø¨Ø±Ù… Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ùˆ Ú†Ù†Ø¯ ØªØ§ Ú©ØªØ§Ø¨ Ø®ÙˆØ¨ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù…."

messages = [
    {"role": "system", "content": "You are a professional Persian to English translator. Translate accurately. Output ONLY English."},
    {"role": "user", "content": f"Translate to English:\n{persian_text}"}
]
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=2048,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
)

input_len = inputs['input_ids'].shape[1]
new_tokens = outputs[0][input_len:]
translation = tokenizer.decode(new_tokens, skip_special_tokens=True)
translation = re.sub(r'<think>.*?</think>', '', translation, flags=re.DOTALL).strip()

print(f"Persian: {persian_text}")
print(f"English: {translation}")
```

---

## ğŸ“š Dataset

### Overview
My models were trained on a highly curated dataset of **300,000 Persian-English sentence pairs**. Instead of relying on massive but noisy datasets, I adopted a "Quality over Quantity" approach, as explored in research on data curation (e.g., [Kreutzer et al., 2018](https://arxiv.org/abs/1805.12282)). I combined filtered web data, high-quality classical translations, and targeted synthetic data to build a robust training set.

### Data Composition & Methodology

The dataset is constructed from three primary sources:

1.  **Filtered CCMatrix (~25% of Source):**
    *   I utilized the [CCMatrix](https://opus.nlpl.eu/CCMatrix/en&fa/v1/CCMatrix) dataset as a base.
    *   Through strict **error-distribution analysis** and scoring with larger teacher models, I filtered out noise and misalignment, keeping only the top **~25%** (the cleanest portion).

<div align="center">
  <img src="images/dataset-01.jpg" alt="Noise Distribution Analysis 1" width="90%"/>
  <br>
  <img src="images/dataset-02.jpg" alt="Noise Distribution Analysis 2" width="90%"/>
  <br>
  <em>Figure: As can be seen, approximately 15 to 35 percent of the data has moderate to very high noise levels.</em>
</div>

2.  **Mizan Subset (~10% of Source):**
    *   I incorporated a carefully selected subset (approx. 10%) of the **Mizan** dataset.
    *   This addition helps the model handle more formal, literary, and classical sentence structures often found in Persian texts.

3.  **High-Quality Synthetic Data (~50K Pairs):**
    *   **The Persian Challenge:** Persian is a low-resource language with complex morphology and flexible word order. Generic models often struggle with these nuances, leading to specific translation weaknesses.
    *   **My Solution:** To address these gaps, I generated approximately **50,000 synthetic sentence pairs** via **Back-Translation**.
    *   This data was not random; it was created to specifically cover the linguistic "blind spots" of standard datasets, significantly improving the model's fluency and its ability to handle complex grammatical structures.

### Dataset Quality Comparison

| Dataset | Size | Quality | Composition | Cleaned |
|---------|------|---------|-------------|---------|
| **Mine (Final)** | **300K** | â­â­â­â­â­ | **Filtered CCMatrix + Mizan + Synthetic** | âœ… **Yes (Manually & Auto)** |
| CCMatrix (Raw) | 25M+ | â­â­ | CommonCrawl Web Data | âŒ No |
| Mizan (Full) | 1M+ | â­â­â­â­ | Literary/Classical | âœ… Yes |
| OPUS-100 | 1M+ | â­â­â­ | Generic Multilingual | âŒ No |

### Download
To reproduce my results or use this curated mixture for your own research, you can download the final processed dataset here:

ğŸ“¥ **[Download Dataset](https://huggingface.co/datasets/MahdiMaaref/PersianToEnglishDataset-1M)**

### Data Cleaning Pipeline

```
Raw Data â†’ Deduplication â†’ Length Filter â†’ Quality Filter â†’ Final Dataset
   â”‚              â”‚              â”‚              â”‚              â”‚
  1.3M+        1.2K           800K           310K           310K
```

### Persian-Specific Challenges Addressed

- âœ… Right-to-Left (RTL) text handling
- âœ… Persian-specific characters and diacritics
- âœ… Informal/colloquial expressions
- âœ… Persian idioms and proverbs
- âœ… Mixed Persian-Arabic script
- âœ… Transliteration of names and places

---


## ğŸ“¦ Model Checkpoints

| Model | Format | Size | Use Case | Download |
|-------|--------|------|----------|----------|
| **ğŸª¶ Lite (0.6B)** | merged (f16) | ~1.2GB | Training/Fine-tuning/Inference | [ğŸ¤— Download](https://huggingface.co/MahdiMaaref/Persian-To-English-Translator) |
| **ğŸ¯ Pro (4B)** | tbd | ~8GB | Training/Fine-tuning/Inference | ğŸš§ *Coming Soon* |

> ğŸ’¡ **Note:** Pro model is currently in development. Stay tuned for updates!
---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 Mahdi Maaref

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...
```

---

<div align="center">

### â­ Star this repo if you find it useful!

**Made with â¤ï¸ for the Persian NLP Community**

[![GitHub stars](https://img.shields.io/github/stars/Mahdi-Maaref/Persian-To-English-Translator?style=social)](https://github.com/Mahdi-Maaref/Persian-To-English-Translator)

</div>
